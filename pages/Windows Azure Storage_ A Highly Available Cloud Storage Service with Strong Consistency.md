title:: Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency
type:: [[Paper]]
conference:: [[SOSP '11]]
doi:: [10.1145/2043556.2043571](https://dl.acm.org/doi/10.1145/2043556.2043571)

- 论文链接
	- [SIGOPS](https://sigops.org/s/conferences/sosp/2011/current/2011-Cascais/printable/11-calder.pdf)
	- [IPFS](https://cloudflare-ipfs.com/ipfs/bafybeieewxuxmbvkn2xhkpxczsgkhk5phs24fd25rtunnq43w4oeikl4ky)
-
- [[Azure Blobs]]，[[Azure Disks]], [[Azure Files]] 等存储服务都是基于同一套架构搭建的
- 除了传统的存储服务之外，这套架构还支持了
	- [[Azure Queues]]
	- [[Azure Tables]]
-
- 先进，非常先进。
- ---
- 关键特性
	- 强一致
	- 全局可共享的命名空间
	- 灾难恢复
	- 多租户和存储成本
		- WAS 对存储成本的理解是比用户在自有硬件上承载相同的负载要更低
- Global Partitioned Namespace
  collapsed:: true
	- WAS 采用全球统一的命名规则
	- `http(s)://AccountName.<service>.core.windows.net/PartitionName/ObjectName`
	- 其中
		- service 指 WAS 提供的各项存储服务，包括 `blob`，`table` 和 `queue`
		- AccountName 是用户的帐户名
		- PartitionName 是一组资源的集合
			- 对应到 [[S3]] 的话，PartitionName 就是 Bucket Name
			- 对 Table 来说就是一张表，对 Queue 来说就是一个队列
			- 现在 [[Azure Storage]] 应该是叫做 Container 了
		- ObjectName 是就是一个特定资源的 Key 的
			- 对应到 [[S3]]，可以叫做 Object Key
	- 跟传统的 [[S3]] 设计是不一样的
		- 一般的 S3 设计不会在全局的路由中加上帐户名
		- Bucket Name 就是全局共享的
		- 国内厂商的话，有 [[COS]] 采用了类似的设计
		- 按照 Paper 中的说法，AccountName 会用来定位主存储集群和数据存储的数据中心
		- 从商业上考虑，云供应商可以为不同的 AccountName 提供不同的服务级别
		- 从实际的体验上来看，AccountName 会跟性能的指标和数据的可用性级别相关
		- 以现在的一个实际创建流程截图为例
			- ![image.png](../assets/image_1641987746628_0.png)
		- Redundancy 的可选项列表
			- ![image.png](../assets/image_1641987806115_0.png)
		- 这个 Account 下的所有资源都会共享相同的配置
	- 突然想到这个设计的一个妙处，这里展开说一下
		- 大多数对象存储系统受到 S3 影响比较大，我们把数据冗余度和数据访问频次混合在了一起，通过 [[存储级别]] 来配置
		- 比如说
			- Standard：标准存储
			- Standard_IA： 低频存储
			- Archive: 归档存储
			- One Zone-IA: 其他的存储级别会在至少三个 AZ 中存储一份，这个存储级别只存储一份，价格会便宜 20%
		- 具体到实现上，大家可能会用三副本来实现 Standard，用
		- 但是 WAS 这里就把这两个维度拆开了，变成了正交的关系
			- Account 级别配置 Redundancy
			- Blobs 级别可以配置 Access Tier
		- 感觉非常的优雅而合理
- High Level Architecture
  collapsed:: true
	- ![image.png](../assets/image_1641989364550_0.png)
	- Storage Stamps
		- 每个 Storage Stamps 是由 N 个机柜组成的集群
		- 每个机柜是一个独立的故障域，有冗余的网络和供电
		- 一般来说每个集群有 10 - 20 个机柜
		- 每个机柜内有 18 个存储节点
		- 每个集群可提供 2PB 左右的原始存储空间
			- 注意这是在 2011 年，硬盘容量普遍是 1 TB
				- *那一年泰国洪水，让机械硬盘大涨价*
			- 现在主流服务器硬盘的大小已经是 8TB/10TB 了，行业内正在突破单盘 20 TB
		- 论文里面还提到空间利用率的问题
			- 为了容量，事务和带宽中取得平衡，WAS 会尽量保证空间利用率在 70% 左右
			- 会尽量避免利用率超过 80%，因为要预留 20% 的空间来防止寻道时间过长和灾备
			- 当空间超过 70% 的时候就会主动触发迁移，将 accounts 迁移到别的 stamp
			- #question 如果单个 Account 存储的数据超过 Stamp 怎么办？
				- 凉拌，看起来设计上就不允许超过
				- [文档](https://docs.microsoft.com/en-us/azure/storage/common/scalability-targets-standard-account) 给出了每个 storage account 的最大配额
					- ![image.png](../assets/image_1641993632655_0.png)
				- 注意这是整个 storage account 共用的
				- 不过 Azure 在文档页高亮了提示，如果超过配额可以找客服申请
					- 我推测应该是会存在更大的 Stamp，可以将这个 Account 迁移过去
	- Location Service (LS)
		- **The location service manages all the storage stamps.**
		- LS 会负责所有 Storage Stamps 的管理，将 Account 映射到 Storage Stamps，处理 Account 在 Stamps 之间的迁移
		- 它本身会在两个地理位置不同的数据中心部署用来做灾备
		- LS 会支持增加新的 Region，新的 Location (感觉上是 AZ 的意思) 和新的 Stamps
		- 论文中给出了一个典型的 Account 创建流程
			- App 请求创建一个新的 Account，请求中会携带指定的 Location
			- LS 会在指定的 Location 选择一个 Stamp 作为该帐号的主 Stamp
				- 选择的算法中会基于所有 Stamp 的负载信息做一些启发式分析
				- 比如说 Stamp 的存储负载，流量和事务的频次
			- 然后 LS 会在这个 Stamp 中存储 Account 相关的 Metadata 信息以通知 Stamp 开始处理这个 Account 的流量
			- 最后 LS 会更新 DNS，将 `http(s)://AccountName.<service>.core.windows.net/` 的解析目标修改为主 Stamp 的 VIP
	- Three Layers within a Storage Stamp
		- Stream Layer
			- **This layer stores the bits on disk and is in charge of distributing and replicating the data across many servers to keep data durable within a storage stamp. **
			- 实际上可以理解为一个跨节点的大号 SSD
			- 它负责对外提供 Stream 的抽象
				- Stream 是由 Extents 组成的有序列表，Extents 表示一组连续的数据 Chunk
				- 处理如何存储，如何复制等问题，不理解上层的抽象和语义
				- 只负责写，读取请求由上层的 Partition Layer 处理
		- Partition Layer
			- 主要功能包括
				- 管理和理解上层业务抽象：Blob，Table，Queue
				- 提供 Object Namespace
				- 为 Object 保证事务顺序和强一致性
				- 在 Stream Layer 之上存储数据
				- 缓存
			- 此外还会提供一些基于 PartitionName 的负载均衡
		- Front-End (FE) layer
			- 一组无状态服务
			- 请求进来的时候完成认证，然后路由到对应的 Partition Server
			- 系统会维护一组 Partion Map，然后 FE 会 cache 这个 map 来提供转发的功能
			- 论文还提到 FE 会从 Stream Layer 直接提供大对象并缓存一些经常访问的数据
				- #question 这个有点奇怪，按照目前的设计，FE 还需要直接从 Stream 读取数据吗？
	- Two Replication Engines
		- Intra-Stamp Replication (stream layer)
			- stream layer 内部的同步 Replication 在 Write IO 的核心路径上
			- Block 粒度(每一笔 IO)
			- 应该是每次写入的时候会写入位于不同节点的多个副本
			- 只有 replication 同步成功，写入请求才会返回成功
		- Inter-Stamp Replication (partition layer)
			- 跨 Stamp 的异步 Replication 不在 IO 核心路径上
			- Object 粒度 (包括跟这个 Object 相关的事务)
			- 用来提供
				- 用户的数据冗余，作为灾备
				- 前面提到的跨 Stamp 迁移 Account
		- Why two ways？
			- 他们针对的是不同场景
			- Intra-Stamp Replication 想处理的是硬件故障
			- 而 Inter-Stamp Replication 想处理的是整个机房挂掉的场景
			- 前者在用户的核心 IO 路径上，所以延迟要求很高；而后者不在，所以只要速度能接受就行
- Stream Layer
	- *重头戏登场*
	- The stream layer provides an internal interface used only by the partition layer.
	- Stream 提供一个类似于 file system 的 API，只不过所有的写入都是 Append Only 的
		- 它支持客户端 open，close，delete，rename，read，append 和 concat
	- 一个 Stream 是一个由 Extent 组成的有序列表，而 Extent 是一组连续的 Append Block
	- ![image.png](../assets/image_1642001477623_0.png)
	- 注意这里的 `Sealed`
		- Sealed Extent 就不能再 Append 新数据了，只有最后一个 Unsealed Extent E4 可以追加新数据
		- 论文明确指出 Stream 只有最后一个 Extent appendable，之前的 Extent 都是 immutable 的
	- Block
		- 读写的最小单元，最大 4MB
		- Client 可以不断的向 Extent 追加不同 size 的 Block
		- 读取数据的时候，不管读多少，整个 block 的内容都会被读取
			- WAS 在 block level 存储一个 checksum，每次读取的时候会校验数据是否正确，防止静默错误
			- 除此以外，整个系统之后每个 block 每隔几天就会被重新校验一下
		- 这个概念实际上是暴露到用户侧的
			- azblob 的用户上传大文件的时候就需要连续上传最大 4MB 的 block
			- 然后最后调用 Put Block List 接口来 commit
			- 最新的版本中将这个限制放开到了 4000 MiB，推测内部的实现应该改了
			- ![image.png](../assets/image_1642002252913_0.png)
	- Extent
		- 复制的最小单元，默认的配置是三副本
			- #question 前面提到 Intra-Stamp Replication 是 Block 粒度的备份，跟这里的描述似乎有些冲突
				- 前文的原话是
					- > intra-stamp replication is focused on replicating blocks of disk storage that are used to make up the objects.
					- 感觉这里的 Block 应该在指代不同的东西
					- `blocks of disk storage` 指磁盘上连续的 block，也就是 Extent，并非上面提到的 Block
		- 每个 Extent 都是一个 [[NTFS]]  文件，包含一组连续的 Block
		- 小文件优化
			- Partiton Layer 会将小文件 append 进同一个 extent 甚至是同一个 block
		- 大文件优化 (Blobs)
			- Partition Layer 会将大文件拆分成不同的 extent
		- Partition Layer 会跟踪每个 Object 由哪个 Stream，Extent 及其 Offset 组成
	- Stream
		- Stream 是一组 Extent **指针** 的有序列表，支持追加写最后一个 extent 和随机读
		- 从已有的 Stream 中的部分内容构造出一个新 Stream 是一个开销很小的操作
			- 因为只需要更新一组指针
			- 这个特性能用于优化 Copy 相关的操作
				- #idea 难怪 [[azblob]] 支持 [Put Blob List](https://docs.microsoft.com/en-us/rest/api/storageservices/put-block-list) 这种操作
					- 先进啊，太先进了
			- 此外灾备的时候也很有用处
		- 只有最后一个 Extent 可写(append)，前面的 Extent 都是 immutable 的
	- Stream Manager and Extent Nodes
		- 接下来看看组成 Stream Layer 的组件
		- ![image.png](../assets/image_1642003395836_0.png)
		- Stream Manager (SM)
			- The SM keeps track of the stream namespace, what extents are in each stream, and the extent allocation across the Extent Nodes (EN).
			- SM 本身是一个 [[Paxos]] 集群，不在读写的关键路径上
			- SM 主要负责的任务
				- 维护 Namespace 和所有活跃的 Streams & Extent
				- 监控所有 EN 健康状态
				- 创建和分配 Extent 到 EN
				- 当固件出故障时执行异步复制
					- 某个 Extent 的副本数不满足要求时
					- 这个操作的时候对故障域会有感知，尽量把不同的副本分配到不同的故障域中
				- GC 没有被引用的 Extent
				- 对 Sealed Extent 执行纠删码
					- #idea 看起来像是某种降低成本的策略
						- 活跃的副本做三副本，提高读写性能
						- 不活跃的，只读的副本转成纠删码，节省存储成本，具体的算法可能后面会介绍
			- SM 只跟踪 Stream 和 Extent，数量级比较少，所有的数据都能直接放在内存中
				- 每个 Stamp 会维护不超过 10W 个 Stream 和不超过 5KW 个 Extent
				- 这些数据能放进 32GB 的内存空间里面
			- #question SM 内存中的数据会落盘吗？
				- 目前还没看到这些细节
		- Extent Nodes (EN)
			- Each extent node maintains the storage for a set of extent replicas assigned to it by the SM.
			- 每个 EN 有 N 块硬盘，完全由这个 EN 管理
			- EN 对 Stream 无感知，只了解 Extent 和 Block
			- Extent 的内部结构
				- 一个 Extent 是一个独立文件(在 NTFS 上)
				- 内部包含数据块以及它的 checksums，此外还有数据块 offset 的索引
			- 每个 EN 会知道它有哪些 Extent 以及他们的 replica 在哪里
			- EN 之间会互相通信来复制客户端的写入或者创建额外的副本
	- Append Operation and Sealed Extent
		- Append 操作是原子的，不存在中间状态
			- #question 怎么做到的？听起来需要底下的文件系统提供一些保证。
			- 如果 append 操作失败了，客户端可以重试(或者 seal 当前的 extent)
				- 相对的，客户端需要能够处理一个 block 被 append 多次的情况
				- 对 metadata 和 commit log stream
					- 每个事务会被分配一个序列号
					- 而重试的这个事务会使用同一个序列号，所以重复的 block 会被 last succeeded write 覆盖
				- 对 row/blob 数据
					- 所以成功写入的数据才会被 RangeParititon 引用
					- 之前的 block 会因为没有被 reference 被 SM GC 掉
				- 后面会更详细的介绍这些逻辑
		- 多个 Block 可以作为一个原子操作同时 Append：“multi-block append”
			- 看起来这个 batch 可以做得很大
		- Stream 的最小读取单元是一个 Block
			- 前文中有提到，每个 block 是有自己的 checksum，每次读的时候会进行校验
		- 每个 extent 会有一个目标大小(由客户端指定)
			- 当这个目标大小达到时(跟 block 的边界对齐)，这个 extent 会被 seal，并创建一个新的 extent
			- 一个 extent seal 之后就变成 immutable 的，SM 可能会对这个 seal 做一些优化
				- 比如说将较冷的 seal 转为纠删码形式进行储存
			- streams 中的 extent 不要求大小一致，他们在任何时候都能被 seal，也能变得非常大
	- Stream Layer Intra-Stamp Replication
		- Partition Layer 的正确性基于 Stream Layer 提供的如下保证
			- 一个记录写入成功之后(append & ack)，从任何一个副本读取这个记录都会获得相同的数据 -> 数据是不可变的
			- 一旦一个 extent 被 seal 之后，从任何一个 sealed 副本读取都会获得相同的数据
				- 注意这里强调了是 sealed replica
				- 也就是说会存在某个时刻，这个 extent 的某个副本还没有被 seal
			- WAS 的数据中心中有其他的安全机制来防范恶意攻击，所以这里不需要考虑这种情况
		- Replication Flow
		  collapsed:: true
			- ![image.png](../assets/image_1642003395836_0.png)
			- 如图所示，当创建 Extent 的时候 (Step A)，SM 会分配一个一主两从的 Extent Replica Set (Step B)
				- 分配好 Extent 之后，这些信息会返回给 client
				- 这个状态会被存储在 SM 元数据中，并缓存在 client 内存里
			- 客户端的写入总是会先落到 Primary EN 上，然后 Primary EN 负责将这个写入复制到从节点上
				- #question 从图中的示意来看，这里采用了链式复制？
				- 当一个 Extent 还没有被 seal 的时候，Primary EN 和他的副本位置保持不变
				- 当客户端的写入收到 Primary EN 返回的 ack 之后，客户端就可以从任意副本中读取数据
				- Primary EN 具体要做的事情包括
					- 决定当前 append 在 extent 中的 offset
						- 最后一个 append 的位置会被当作当前副本的 **commit length**
					- 对 append 请求排序(如果存在并发的乱序 append)
					- 把 append 和被选中的 offset 发送给从节点
					- 当且仅当三个节点都写入成功时返回 ack
			- 当 Streams 中的 extent 被 seal 之后，这个流程会再次重复
			- 前面提到 Extent 的相关信息会被 Client 缓存，所以 Client 会直接向 ENs 发送请求，直到需要创建新的 Extent
			- 如果在写入的时候发生失败，Client 会跟 SM 通信，SM 会将当前的 extent 以现在的 commit length seal 并创建一个新的 extent (在其他的可用的 EN 上)
				- 整个过程平均会在 20ms 内完成
					- #idea 好快啊。。。
				- 关键点在于这个客户端可以立即开始写一个新的 extent，不需要等待特定的节点恢复
				- 与此同时，SM 会为刚刚被 seal 的 extent 创建一个新的副本，从而使得它满足冗余度的要求
		- Sealing
			- Seal 一个 Extent 的时候，SM 首先会向三个 EN 询问指定 extent 当前的 commit length
			- 然后 SM 会选择可用的 EN 中最小的 commit length
				- 考虑前面的正确性保证，大于这个 comment length 的写入都没有返回 ack
				- 相反的，在这个 commit length 之前的所有写入都已经正常返回了 ack
					- #idea 考虑前面的客户端失败的处理，如果写入返回了 ack 但是客户端没有收到，此时客户端会 retry 这个请求。
				- 所以这一步中不会出现数据丢失
			- 所有 SM 可访问的 EN 都会以 SM 选择的 commit length 进行 seal
			- seal 完成后，这个 commit length 就是不可变的
			- 如果某个 EN 在 seal 的时候无法访问但是随后又自己恢复了，SM 会强迫这个 EN 将指定的 Extent 同步到指定的 commit length 并标记为 sealed
			- 所以所有的 Extent 副本最终都会达到完全一致的状态
		- Interaction with Partition Layer
			- 这个小章节讨论脑裂的情况：SM 无法访问指定的 EN，但是 Client 访问仍然正常
			- #question 这个没读懂
	- Erasure Coding Sealed Extents
		- WAS 会运用 [[纠删码]] 来节省 Extent 的存储空间
			- 通常来说，[[纠删码]] 相比于三副本能够节省一半以上的空间 ( 3x -> 1.3x ~ 1.5x)
			- 而且相比于三副本，纠删码实际上更能提升数据的持久性
				- #question 论文这里没有将采用纠删码后这个 extent 在 stamp 内是怎么分布的，看起来是这些块会分布在不同的 SN 上？
		- WAS 仅在 Blob Storage 上采用这项优化，应该是权衡性能和成本的结果
	- Read Load-Balancing
		- Client 在发起读取请求的时候会指定一个 deadline，如果 EN 满足不了这个 deadline 就会立刻返回，不再尝试
			- #question 这个 deadline 具体是指哪个阶段的 deadline？
			- #question 读取流程是分二阶段的吗？先发送预期的 deadline，能满足才发送真正的 read 请求？
		- 这个优化也同样应用于纠删码处理之后的 Extent
			- 如果读取已有的分片无法满足 deadline，client 会向所有的分片发送读取请求，然后使用最先返回的 N 个分片来重新构造出请求需要的数据
	- Spindle Anti-Starvation
		- 这个小章节介绍了硬盘公平调度的一个优化
		- 当硬盘的队列过深的时候，可能会导致 IO 超过 2300ms 没有返回
		- 所以 WAS 开发了自己的 IO 调度器，保证
			- 当硬盘已经有超过 100ms 的待处理请求被调度
			- 或者硬盘已经
- ---
- 无用但有趣的一些小发现
	- WAS 很容易手滑打成 AWS (
		- 后来 WAS 把前面的 windows 去掉了，只说 Azure Storage
		- 肯定跟这个没关系 (
- [[2022-01-14]] 感想
	- 有种逐渐理解了一切的感觉， [[azblob]] 之前有些奇怪的设计都有了合理的解释